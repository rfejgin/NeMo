{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import T5TTS_Model\n",
    "from nemo.collections.tts.data.text_to_speech_dataset import T5TTSDataset, DatasetSample\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = \"/data/t5_new_cp/configs/unnormalizedLalign005_singleencoder_kernel3_hparams.yaml\"\n",
    "checkpoint_file = \"/data/t5_new_cp/checkpoints/unnormalizedLalign005_singleencoder_kernel3_epoch_20.ckpt\" #\"/datap/misc/continuouscheckpoints/edresson_epoch21.ckpt\"\n",
    "out_dir = \"/datap/misc/t5tts_inference_notebook_samples\"\n",
    "#codecmodel_path = \"/datap/misc/checkpoints/AudioCodec_21Hz_no_eliz.nemo\"\n",
    "codecmodel_path = \"/data/codec_checkpoints/codecs-no-eliz/AudioCodec_21Hz_no_eliz_without_wavlm_disc.nemo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = OmegaConf.load(hparams_file).cfg\n",
    "\n",
    "with open_dict(model_cfg):\n",
    "    model_cfg.codecmodel_path = codecmodel_path\n",
    "    if hasattr(model_cfg, 'text_tokenizer'):\n",
    "        # Backward compatibility for models trained with absolute paths in text_tokenizer\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_dict = \"scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt\"\n",
    "        model_cfg.text_tokenizer.g2p.heteronyms = \"scripts/tts_dataset_files/heteronyms-052722\"\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_probability = 1.0\n",
    "    model_cfg.train_ds = None\n",
    "    model_cfg.validation_ds = None\n",
    "\n",
    "\n",
    "model = T5TTS_Model(cfg=model_cfg)\n",
    "# Load weights from checkpoint file\n",
    "print(\"Loading weights from checkpoint\")\n",
    "ckpt = torch.load(checkpoint_file)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "print(\"Loaded weights.\")\n",
    "\n",
    "if model_cfg.t5_decoder.pos_emb == \"learnable\":\n",
    "    if (model_cfg.t5_decoder.use_flash_self_attention) is False and (model_cfg.t5_decoder.use_flash_self_attention is False):\n",
    "        print(\"Using kv cache for inference.\")\n",
    "        model.use_kv_cache_for_inference = True\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843167ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = T5TTSDataset(\n",
    "    dataset_meta={},\n",
    "    sample_rate=model_cfg.sample_rate,\n",
    "    min_duration=0.5,\n",
    "    max_duration=20,\n",
    "    codec_model_downsample_factor=model_cfg.codec_model_downsample_factor,\n",
    "    bos_id=model.bos_id,\n",
    "    eos_id=model.eos_id,\n",
    "    context_audio_bos_id=model.context_audio_bos_id,\n",
    "    context_audio_eos_id=model.context_audio_eos_id,\n",
    "    audio_bos_id=model.audio_bos_id,\n",
    "    audio_eos_id=model.audio_eos_id,\n",
    "    num_audio_codebooks=model_cfg.num_audio_codebooks,\n",
    "    prior_scaling_factor=None,\n",
    "    load_cached_codes_if_available=True,\n",
    "    dataset_type='test',\n",
    "    tokenizer_config=None,\n",
    "    load_16khz_audio=model.model_type == 'single_encoder_sv_tts',\n",
    "    use_text_conditioning_tokenizer=model.use_text_conditioning_encoder,\n",
    "    pad_context_text_to_max_duration=model.pad_context_text_to_max_duration,\n",
    "    context_duration_min=model.cfg.get('context_duration_min', 5.0),\n",
    "    context_duration_max=model.cfg.get('context_duration_max', 5.0),\n",
    ")\n",
    "test_dataset.text_tokenizer, test_dataset.text_conditioning_tokenizer = model._setup_tokenizers(model.cfg, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7374d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_jhsd = True\n",
    "if not do_jhsd:\n",
    "    audio_dir =  \"/datap/misc/RodneyLindy44\" #\"/datap/misc/Datasets/riva\"\n",
    "    entry = {\n",
    "        #\"audio_filepath\": \"Lindy/22khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "        \"audio_filepath\": \"Lindy/44khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "        \"duration\": 6.275604,\n",
    "        \"text\": \"This is a very long sentence to test the speed of my text to speech synthesis model, let's see how long it takes to generate the audio. Adding more words to make it even longer. Haha.\",\n",
    "        \"speaker\": \"dummy\",\n",
    "    #     \"context_text\": \"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_CMU_FEARFUL |\",\n",
    "        \"context_audio_filepath\": \"Rodney/44khz/DROP/RODNEY_DROP_000060.wav\", #\"Rodney/22khz/DROP/RODNEY_DROP_000060.wav\",\n",
    "        \"context_audio_duration\": 8.0,\n",
    "    }\n",
    "else:\n",
    "    # Jensen data\n",
    "    audio_dir =  \"/data/NV-RESTRICTED/JHSD/22khz\"\n",
    "    entry = {\n",
    "        #\"audio_filepath\": \"Lindy/22khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "        \"audio_filepath\": \"AMP20_KEYNOTE-VOOnly-44khz-16bit-mono_17.wav\",\n",
    "        \"duration\": 4.84,\n",
    "        \"text\": \"This is a very long sentence to test the speed of my text to speech synthesis model, let's see how long it takes to generate the audio. Adding more words to make it even longer. Haha.\",\n",
    "        \"speaker\": \"dummy\",\n",
    "    #     \"context_text\": \"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_CMU_FEARFUL |\",\n",
    "        #\"context_audio_filepath\": \"GTC20_FALL_KEYNOTE-VOOnly-44khz-16bit-mono_221.wav\",\n",
    "        \"context_audio_filepath\": \"AMP20_KEYNOTE-VOOnly-44khz-16bit-mono_6.wav\", #\"Rodney/22khz/DROP/RODNEY_DROP_000060.wav\",\n",
    "        #\"context_audio_duration\": 5.35,# 8.02,\n",
    "        \"context_audio_duration\": 8.02,\n",
    "    }\n",
    "data_sample = DatasetSample(\n",
    "    dataset_name=\"sample\",\n",
    "    manifest_entry=entry,\n",
    "    audio_dir=audio_dir,\n",
    "    feature_dir=audio_dir,\n",
    "    text=entry['text'],\n",
    "    speaker=None,\n",
    "    speaker_index=0\n",
    ")\n",
    "test_dataset.data_samples = [data_sample]\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_idx = 0\n",
    "for bidx, batch in enumerate(test_data_loader):\n",
    "    print(\"Processing batch {} out of {}\".format(bidx, len(test_data_loader)))\n",
    "    model.t5_decoder.reset_cache(use_cache=True)\n",
    "    batch_cuda ={}\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch_cuda[key] = batch[key].cuda()\n",
    "        else:\n",
    "            batch_cuda[key] = batch[key]\n",
    "    import time\n",
    "    st = time.time()\n",
    "    predicted_audio, predicted_audio_lens, _, _ = model.infer_batch(batch_cuda, max_decoder_steps=500, temperature=0.5, topk=80)\n",
    "    print(\"generation time\", time.time() - st)\n",
    "    for idx in range(predicted_audio.size(0)):\n",
    "        predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "        predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "        audio_path = os.path.join(out_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "        sf.write(audio_path, predicted_audio_np, model.cfg.sample_rate)\n",
    "        display(Audio(audio_path))\n",
    "        item_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_filepath = os.path.join(audio_dir, entry['context_audio_filepath'])\n",
    "display(Audio(context_filepath))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
