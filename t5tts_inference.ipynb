{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import T5TTS_Model\n",
    "from nemo.collections.tts.data.text_to_speech_dataset import T5TTSDataset, DatasetSample\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5798ac",
   "metadata": {},
   "source": [
    "### Checkpoint Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_file = \"/data/t5_new_cp/configs/unnormalizedLalign005_singleencoder_kernel3_hparams.yaml\"\n",
    "checkpoint_file = \"/data/t5_new_cp/checkpoints/unnormalizedLalign005_singleencoder_kernel3_epoch_20.ckpt\" #\"/datap/misc/continuouscheckpoints/edresson_epoch21.ckpt\"\n",
    "out_dir = \"/datap/misc/t5tts_inference_notebook_samples\"\n",
    "#codecmodel_path = \"/datap/misc/checkpoints/AudioCodec_21Hz_no_eliz.nemo\"\n",
    "codecmodel_path = \"/data/codec_checkpoints/codecs-no-eliz/AudioCodec_21Hz_no_eliz_without_wavlm_disc.nemo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hparams_file = \"yt_weight_0.25_plus18k__dim1536__enc3_fixes_hparams.yaml\"\n",
    "#checkpoint_file = \"yt_weight_0.25_plus18k__dim1536__enc3_fixes_val_loss_5.1870_epoch_25.ckpt\"\n",
    "\n",
    "#hparams_file = \"/data/t5_new_cp/configs/unnormalizedLalign005_singleencoder_kernel3_hparams.yaml\"\n",
    "#checkpoint_file = \"/data/t5_new_cp/checkpoints/unnormalizedLalign005_singleencoder_kernel3_epoch_20.ckpt\" #\"/datap/misc/continuouscheckpoints/edresson_epoch21.ckpt\"\n",
    "hparams_file = \"/datap/misc/continuouscheckpoints/yt_weight0.25_plus_18k_single_stage_decoder_context_kernel1_fixes_hparams.yaml\"\n",
    "checkpoint_file =\"/datap/misc/continuouscheckpoints/yt_weight0.25_plus_18k_single_stage_decoder_context_kernel1_fixes_epoch_61.ckpt\" \n",
    "#hparams_file = \"/datap/misc/continuouscheckpoints/decoder_context_large_hparams.yaml\"\n",
    "#checkpoint_file =\"/datap/misc/continuouscheckpoints/decoder_context_large_epoch_14.ckpt\" \n",
    "\n",
    "out_dir = \"inference_output__adi_prompt_v2\"\n",
    "#codecmodel_path = \"/datap/misc/checkpoints/AudioCodec_21Hz_no_eliz.nemo\"\n",
    "codecmodel_path = \"/data/codec_checkpoints/codecs-no-eliz/AudioCodec_21Hz_no_eliz_without_wavlm_disc.nemo\"\n",
    "\n",
    "model_cfg = OmegaConf.load(hparams_file).cfg\n",
    "\n",
    "with open_dict(model_cfg):\n",
    "    model_cfg.codecmodel_path = codecmodel_path\n",
    "    if hasattr(model_cfg, 'text_tokenizer'):\n",
    "        # Backward compatibility for models trained with absolute paths in text_tokenizer\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_dict = \"scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt\"\n",
    "        model_cfg.text_tokenizer.g2p.heteronyms = \"scripts/tts_dataset_files/heteronyms-052722\"\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_probability = 1.0\n",
    "    model_cfg.train_ds = None\n",
    "    model_cfg.validation_ds = None\n",
    "\n",
    "\n",
    "model = T5TTS_Model(cfg=model_cfg)\n",
    "print(\"Loading weights from checkpoint\")\n",
    "ckpt = torch.load(checkpoint_file)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "print(\"Loaded weights.\")\n",
    "\n",
    "if model_cfg.t5_decoder.pos_emb == \"learnable\":\n",
    "    if (model_cfg.t5_decoder.use_flash_self_attention) is False and (model_cfg.t5_decoder.use_flash_self_attention is False):\n",
    "        print(\"Using kv cache for inference.\")\n",
    "        model.use_kv_cache_for_inference = True\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b5711",
   "metadata": {},
   "source": [
    "### Initialize Dataset class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = T5TTSDataset(\n",
    "    dataset_meta={},\n",
    "    sample_rate=model_cfg.sample_rate,\n",
    "    min_duration=0.5,\n",
    "    max_duration=20,\n",
    "    codec_model_downsample_factor=model_cfg.codec_model_downsample_factor,\n",
    "    bos_id=model.bos_id,\n",
    "    eos_id=model.eos_id,\n",
    "    context_audio_bos_id=model.context_audio_bos_id,\n",
    "    context_audio_eos_id=model.context_audio_eos_id,\n",
    "    audio_bos_id=model.audio_bos_id,\n",
    "    audio_eos_id=model.audio_eos_id,\n",
    "    num_audio_codebooks=model_cfg.num_audio_codebooks,\n",
    "    prior_scaling_factor=None,\n",
    "    load_cached_codes_if_available=True,\n",
    "    dataset_type='test',\n",
    "    tokenizer_config=None,\n",
    "    load_16khz_audio=model.model_type == 'single_encoder_sv_tts',\n",
    "    use_text_conditioning_tokenizer=model.use_text_conditioning_encoder,\n",
    "    pad_context_text_to_max_duration=model.pad_context_text_to_max_duration,\n",
    "    context_duration_min=model.cfg.get('context_duration_min', 5.0),\n",
    "    context_duration_max=model.cfg.get('context_duration_max', 5.0),\n",
    ")\n",
    "test_dataset.text_tokenizer, test_dataset.text_conditioning_tokenizer = model._setup_tokenizers(model.cfg, mode='test')\n",
    "\n",
    "\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    with sf.SoundFile(file_path) as audio_file:\n",
    "        # Calculate the duration\n",
    "        duration = len(audio_file) / audio_file.samplerate\n",
    "        return duration\n",
    "\n",
    "def create_record(text, context_audio_filepath=None, context_text=None):\n",
    "    dummy_audio_fp = os.path.join(out_dir, \"dummy_audio.wav\")\n",
    "    dummy_audio = sf.write(dummy_audio_fp, np.zeros(22050 * 3), 22050)  # 3 seconds of silence\n",
    "    record = {\n",
    "        'audio_filepath' : dummy_audio_fp,\n",
    "        'duration': 3.0,\n",
    "        'text': text,\n",
    "        'speaker': \"dummy\",\n",
    "    }\n",
    "    if context_text is not None:\n",
    "        assert context_audio_filepath is None\n",
    "        record['context_text'] = context_text\n",
    "    else:\n",
    "        assert context_audio_filepath is not None\n",
    "        record['context_audio_filepath'] = context_audio_filepath\n",
    "        record['context_audio_duration'] = get_audio_duration(context_audio_filepath)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa7a5a",
   "metadata": {},
   "source": [
    "### Set transcript and context pairs to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74683d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "usg_cfg = True\n",
    "cfg_scale = 1.8\n",
    "audio_dir = \"/home/rfejgin/kb-snippets\"\n",
    "texts = [\"NVIDIA's Riva is a powerful speech AI toolkit that offers state-of-the-art ASR and TTS capabilities.\",\n",
    " 'The platform supports multiple languages and provides enterprise-grade speech technology through GPU-accelerated SDKs and APIs.',\n",
    " 'What makes Riva unique is its ability to be customized for specific use cases while maintaining high performance and accuracy.',\n",
    " 'The platform supports both cloud and edge deployment, making it versatile for various enterprise applications.',\n",
    " 'When comparing Heavenly and Northstar ski resorts in Lake Tahoe, each offers unique advantages.',\n",
    " 'Heavenly boasts more vertical feet of skiing and spectacular lake views, with 4,800 acres of skiable terrain across two states.',\n",
    " 'Northstar offers a more intimate, luxury experience with excellent grooming, a charming village atmosphere, and some of the best tree skiing in Tahoe.',\n",
    " 'Heavenly might be better for advanced skiers seeking variety, while Northstar excels for families and intermediate skiers looking for a refined experience.',\n",
    " \"Being a Product Manager at NVIDIA is unique because you're at the forefront of AI innovation, working with cutting-edge technology that shapes the future of computing.\",\n",
    " 'The role combines technical depth with market strategy, requiring understanding of both deep learning models and enterprise customer needs.',\n",
    " 'You get to collaborate with world-class researchers and engineers while driving products that enable breakthrough AI applications across industries.',\n",
    " 'The boy was there when the sun rose.',\n",
    " 'A rod is used to catch pink salmon.',\n",
    " 'The source of the huge river is the clear spring.',\n",
    " 'Kick the ball straight and follow through.',\n",
    " 'Help the woman get back to her feet.',\n",
    " 'A pot of tea helps to pass the evening.',\n",
    " 'Smoky fires lack flame and heat.',\n",
    " \"The soft cushion broke the man's fall.\",\n",
    " 'The salt breeze came across from the sea.',\n",
    " 'The girl at the booth sold fifty bonds.']\n",
    "entries = []\n",
    "for i,text in enumerate(texts):\n",
    "    entry = {\"audio_filepath\": \"adi-snippet1.wav\",\n",
    "                \"duration\": 4.89,\n",
    "                \"text\": text,\n",
    "                \"speaker\": \"dummy\",\n",
    "                \"context_audio_filepath\": \"adi-snippet1.wav\",\n",
    "                \"context_audio_duration\":  4.89\n",
    "    }\n",
    "    entries.append(entry)\n",
    "data_samples = [DatasetSample(\n",
    "    dataset_name=\"sample\",\n",
    "    manifest_entry=entry,\n",
    "    audio_dir=audio_dir,\n",
    "    feature_dir=audio_dir,\n",
    "    text=entry['text'],\n",
    "    speaker=None,\n",
    "    speaker_index=0\n",
    ") for entry in entries]\n",
    "num_repeat = 5\n",
    "test_dataset.data_samples = data_samples\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=6,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7374d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    text_index = 0\n",
    "    texts = [\"Welcome to the conversational AI face-to-face meeting. I hope you're enjoying it and learning a lot. By the way, I never said any of this.\" ,\n",
    "            # text generated by ChatGPT\n",
    "            \"Our GPUs arenâ€™t just processors; they are engines for discovery, powering breakthroughs in everything from self-driving cars to disease research.\",\n",
    "            \n",
    "            # text from t5-tts paper\n",
    "            \"Experiments demonstrate that our alignment learning procedure improves the reliability of TTS synthesis, especially for challenging text inputs and outperforms prior LLM-based TTS models on both intelligibility and naturalness.\", # from t5-tts\n",
    "            \n",
    "            # from KB interview\n",
    "            \"The book is about seeing different things and finding similarities. Each kid in the book looks a little bit different, but also a little bit the same.\",\n",
    "\n",
    "            # from NVIDIA website\n",
    "            \"Speech AI lets people converse with devices, machines, and computers to simplify and augment their lives.\", # from Nvidia website\n",
    "            \n",
    "            # text from CES keynote\n",
    "            \"And today, computing is revolutionized in every single layer, from hand coding, instructions that run on CPUs to create software tools that humans use.\", # text from CES keynote\n",
    "\n",
    "            # text from CES keynote\n",
    "            \"And so we have models for vision, for understanding languages, for speech, for animation, for digital biology, and we have some new exciting models coming for physical AI.\",\n",
    "            #  \"One amazing AI world foundation model, the world's first physical AI foundation model is open, available to activate the world's industries of robotics and such, and three robots working on agentic AI, humanoid robots, and self-driving cars.\", # text from CES keynote\n",
    "            #  \"That's all going to be either highly autonomous or fully autonomous coming up. And so this is going to be a very large industry.\" # text from CES keynote\n",
    "            ]\n",
    "\n",
    "    do_jhsd = True\n",
    "    use_cfg = True\n",
    "    cfg_scale = 1.8\n",
    "    kb=True\n",
    "    sqam=True\n",
    "    if kb:\n",
    "        audio_dir = \"/home/rfejgin/kb-snippets\"\n",
    "        #audio_dir = \"/datap/misc/RodneyLindy44/Lindy/44khz/WIZWIKI\"\n",
    "        entry = {\n",
    "                #\"audio_filepath\": \"Lindy/22khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "                #\"audio_filepath\": \"LINDY_WIZWIKI_007592.wav\",#\"kb-snippet-bahamas.wav\",\n",
    "                #\"audio_filepath\":\"boris-snippet-denoised.wav\",\n",
    "                \"audio_filepath\":\"adi-snippet1.wav\",\n",
    "                \"duration\": 9.54,\n",
    "                #\"text\": \"Speech AI lets people converse with devices, machines, and computers to simplify and augment their lives.\",\n",
    "                \"text\": texts[text_index],\n",
    "                \"speaker\": \"dummy\",\n",
    "            #     \"context_text\": \"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_CMU_FEARFUL |\",\n",
    "                \"context_audio_filepath\": \"adi-snippet1.wav\", #\"LINDY_WIZWIKI_007592.wav\",#\"siddhesh.wav\", #\"Rodney/22khz/DROP/RODNEY_DROP_000060.wav\",\n",
    "                \"context_audio_duration\": 4.89,\n",
    "            }\n",
    "    elif sqam:\n",
    "        audio_dir = \"/home/rfejgin/t5-util/mos\"\n",
    "        entry = {\n",
    "                #\"audio_filepath\": \"Lindy/22khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "                \"audio_filepath\": \"sqam_cd_49_short_mono.wav\",\n",
    "                \"duration\": 6.44,\n",
    "                \"text\": texts[text_index],\n",
    "                \"speaker\": \"dummy\",\n",
    "            #     \"context_text\": \"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_CMU_FEARFUL |\",\n",
    "                \"context_audio_filepath\": \"sqam_cd_49_short_mono.wav\", #\"Rodney/22khz/DROP/RODNEY_DROP_000060.wav\",\n",
    "                \"context_audio_duration\": 6.44,\n",
    "            }  \n",
    "    else:\n",
    "        if not do_jhsd:\n",
    "            audio_dir =  \"/datap/misc/RodneyLindy44\" #\"/datap/misc/Datasets/riva\"\n",
    "            entry = {\n",
    "                #\"audio_filepath\": \"Lindy/22khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "                \"audio_filepath\": \"Lindy/44khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "                \"duration\": 6.275604,\n",
    "                \"text\": texts[text_index],\n",
    "                \"speaker\": \"dummy\",\n",
    "            #     \"context_text\": \"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_CMU_FEARFUL |\",\n",
    "                \"context_audio_filepath\": \"Rodney/44khz/DROP/RODNEY_DROP_000060.wav\", #\"Rodney/22khz/DROP/RODNEY_DROP_000060.wav\",\n",
    "                \"context_audio_duration\": 8.0,\n",
    "            }\n",
    "        else:\n",
    "\n",
    "            # Jensen data\n",
    "            #audio_dir =  \"/data/NV-RESTRICTED/JHSD/22khz\"\n",
    "            audio_dir =  \"/data/NV-RESTRICTED/JHSD/22khz_denoised\"\n",
    "            entry = {\n",
    "                #\"audio_filepath\": \"Lindy/22khz/CMU_HAPPY/LINDY_CMU_HAPPY_000567.wav\",\n",
    "                #\"audio_filepath\": \"GTC_FALL_2021_KEYNOTE_V0Only-44khz-16bit-mono_CH07_0042.wav\",\n",
    "                \"audio_filepath\": \"GTC20_SPRING_KEYNOTE-VOOnly-44khz-16bit-mono_327.wav\",\n",
    "                \"duration\": 4.84,\n",
    "                \"text\": texts[text_index],\n",
    "                #\"text\": \"It was simply amazing to watch. I've never seen collaboration on such scale.\",\n",
    "                \"speaker\": \"not used\",\n",
    "            #     \"context_text\": \"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_CMU_FEARFUL |\",\n",
    "                \"context_audio_filepath\": \"GTC_FALL_2021_KEYNOTE_V0Only-44khz-16bit-mono_CH07_0042.wav\",\n",
    "                #\"context_audio_filepath\": \"GTC20_FALL_KEYNOTE-VOOnly-44khz-16bit-mono_221.wav\",\n",
    "                #\"context_audio_filepath\": \"roy2_22050.wav\",#\"AMP20_KEYNOTE-VOOnly-44khz-16bit-mono_6.wav\", #\"Rodney/22khz/DROP/RODNEY_DROP_000060.wav\",\n",
    "                #\"context_audio_duration\": 5.35,# 8.02,\n",
    "                \"context_audio_duration\": 6.24,\n",
    "            }\n",
    "    data_sample = DatasetSample(\n",
    "        dataset_name=\"sample\",\n",
    "        manifest_entry=entry,\n",
    "        audio_dir=audio_dir,\n",
    "        feature_dir=audio_dir,\n",
    "        text=entry['text'],\n",
    "        speaker=None,\n",
    "        speaker_index=0\n",
    "    )\n",
    "    num_repeat = 5\n",
    "    test_dataset.data_samples = [data_sample for _ in range(num_repeat)]\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=6,\n",
    "        collate_fn=test_dataset.collate_fn,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_idx = 0\n",
    "usg_cfg = True\n",
    "for bidx, batch in enumerate(test_data_loader):\n",
    "    print(\"Processing batch {} out of {}\".format(bidx, len(test_data_loader)))\n",
    "    model.t5_decoder.reset_cache(use_cache=True)\n",
    "    batch_cuda ={}\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch_cuda[key] = batch[key].cuda()\n",
    "        else:\n",
    "            batch_cuda[key] = batch[key]\n",
    "    import time\n",
    "    st = time.time()\n",
    "    use_cfg = True\n",
    "    predicted_audio, predicted_audio_lens, _, _ = model.infer_batch(batch_cuda, max_decoder_steps=500, temperature=0.5, topk=80,\\\n",
    "                                                                    use_cfg=use_cfg, cfg_scale=cfg_scale)\n",
    "    print(\"generation time\", time.time() - st)\n",
    "    for idx in range(predicted_audio.size(0)):\n",
    "        predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "        predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "        model_name = os.path.basename(checkpoint_file)\n",
    "        #audio_path = os.path.join(out_dir, f\"text_{text_index}__predicted_audio_{item_idx}.wav\")\n",
    "        audio_path = os.path.join(out_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "        print(f\"Writing {audio_path}\")\n",
    "        sf.write(audio_path, predicted_audio_np, model.cfg.sample_rate)\n",
    "        display(Audio(audio_path))\n",
    "        item_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Checkpoint: {checkpoint_file}\")\n",
    "context_filepath = os.path.join(audio_dir, entry['context_audio_filepath'])\n",
    "display(Audio(context_filepath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry['context_audio_filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0418e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "usg_cfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
